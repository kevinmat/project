{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XEwMLf6uGC17"
   },
   "source": [
    "# CNN Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l99gQAFOdfxd"
   },
   "source": [
    "## Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "A5brY9NpHJ8E",
    "outputId": "a3696105-0874-41ea-beef-7f7a531be39c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "from skimage import measure\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ruNrdPe7dlEL"
   },
   "source": [
    "## Split into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ipwLjkKTHh9Q",
    "outputId": "01939d7b-e8cd-4bf0-e1bb-403884c9f73b"
   },
   "outputs": [],
   "source": [
    "#TRAIN_PATH = '/content/sample_data/stage_2_train_images/'\n",
    "\n",
    "\n",
    "filenames = {}\n",
    "\n",
    "filenames = os.listdir(PROJECT_PATH + \"/stage_2_train_images\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train data and Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files available: 26684\n",
      "n train samples 18679\n",
      "n valid samples 8005\n",
      "Image Dimension to use: 128\n",
      "sample file: 095ce5a1-3928-4120-9f15-24162c7f27c7.dcm\n"
     ]
    }
   ],
   "source": [
    "file_count = int(len(filenames))\n",
    "print(\"Total files available:\",file_count)\n",
    "\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# split into train and validation\n",
    "n_valid_samples = int(file_count * 0.3)\n",
    "\n",
    "train_filenames = filenames[n_valid_samples:file_count]\n",
    "valid_filenames = filenames[:n_valid_samples]\n",
    "print('n train samples', len(train_filenames))\n",
    "print('n valid samples', len(valid_filenames))\n",
    "n_train_samples = len(filenames) - n_valid_samples\n",
    "\n",
    "image_dimension = 128\n",
    "print('Image Dimension to use:',image_dimension)\n",
    "print('sample file:',filenames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z-yI9i_dxLb"
   },
   "source": [
    "## Read stage_2_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HsUvrbnLS6Aw"
   },
   "outputs": [],
   "source": [
    "# empty dictionary\n",
    "pneumonia_locations = {}\n",
    "# load table\n",
    "with open(PROJECT_PATH + '/stage_2_train_labels.csv', mode='r') as infile:\n",
    "    # open reader\n",
    "    reader = csv.reader(infile)\n",
    "    # skip header\n",
    "    next(reader, None)\n",
    "    # loop through rows\n",
    "    for rows in reader:\n",
    "        # retrieve information\n",
    "        filename = rows[0]\n",
    "        location = rows[1:5]\n",
    "        pneumonia = rows[5]\n",
    "        # if row contains pneumonia add label to dictionary\n",
    "        # which contains a list of pneumonia locations per filename\n",
    "        if pneumonia == '1':\n",
    "            # convert string to float to int\n",
    "            location = [int(float(i)) for i in location]\n",
    "            # save pneumonia location in dictionary\n",
    "            if filename in pneumonia_locations:\n",
    "                pneumonia_locations[filename].append(location)\n",
    "            else:\n",
    "                pneumonia_locations[filename] = [location]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XxKD9c8d3VB"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Qg5IaxkzGCNg",
    "outputId": "a709ecf5-cb2f-4e0f-e4f1-01a0ac1b72d4"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# The dataset is too large to fit into memory, so we need to create a generator that loads data on the fly.\n",
    "# Generator class to handle:\n",
    "# Image load from folder during train and predict modes, shuffle on epoc end, \n",
    "# resize loaded images, augment if needed, add trailing channel dimension\n",
    "class generator(keras.utils.all_utils.Sequence):\n",
    "    \n",
    "    def __init__(self, folder, filenames, pneumonia_locations=None, batch_size=32, image_size=image_dimension, shuffle=True, augment=False, predict=False):\n",
    "        self.folder = folder\n",
    "        self.filenames = filenames\n",
    "        self.pneumonia_locations = pneumonia_locations\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        self.predict = predict\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    # Loads the file from folder, resizes and augments the data with horizontal flip    \n",
    "    def __load__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        #print('reading file:', filename)\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename), force=True).pixel_array\n",
    "\n",
    "        # create empty mask\n",
    "        msk = np.zeros(img.shape)\n",
    "        # get filename without extension\n",
    "        filename = filename.split('.')[0]\n",
    "        # if image contains pneumonia\n",
    "        if filename in self.pneumonia_locations:\n",
    "            # loop through pneumonia\n",
    "            for location in self.pneumonia_locations[filename]:\n",
    "                # add 1's at the location of the pneumonia\n",
    "                x, y, w, h = location\n",
    "                msk[y:y+h, x:x+w] = 1\n",
    "        # resize both image and mask\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        msk = resize(msk, (self.image_size, self.image_size), mode='reflect') > 0.5\n",
    "        # if augment then horizontal flip half the time\n",
    "        if self.augment and random.random() > 0.5:\n",
    "            img = np.fliplr(img)\n",
    "            msk = np.fliplr(msk)\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        msk = np.expand_dims(msk, -1)\n",
    "        return img, msk\n",
    "    \n",
    "    # Loads images during prediction cycles\n",
    "    def __loadpredict__(self, filename):\n",
    "        # load dicom file as numpy array\n",
    "        # print('reading file:', filename)\n",
    "        img = pydicom.dcmread(os.path.join(self.folder, filename), force=True).pixel_array\n",
    "        \n",
    "        # resize image\n",
    "        img = resize(img, (self.image_size, self.image_size), mode='reflect')\n",
    "        # add trailing channel dimension\n",
    "        img = np.expand_dims(img, -1)\n",
    "        return img\n",
    "        \n",
    "    # Generator must implement this getter function    \n",
    "    def __getitem__(self, index):\n",
    "        # select batch\n",
    "        filenames = self.filenames[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # predict mode: return images and filenames\n",
    "        if self.predict:\n",
    "            # load files\n",
    "            imgs = [self.__loadpredict__(filename) for filename in filenames]\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            return imgs, filenames\n",
    "        # train mode: return images and masks\n",
    "        else:\n",
    "            # load files\n",
    "            items = [self.__load__(filename) for filename in filenames]\n",
    "            # unzip images and masks\n",
    "            imgs, msks = zip(*items)\n",
    "            # create numpy batch\n",
    "            imgs = np.array(imgs)\n",
    "            msks = np.array(msks)\n",
    "            return imgs, msks\n",
    "\n",
    "    # Shuffle data before start of next epoc    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.filenames)\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.predict:\n",
    "            # return everything\n",
    "            return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "        else:\n",
    "            # return full batches only\n",
    "            return int(len(self.filenames) / self.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V8Rpzzknd8Zu"
   },
   "source": [
    "## Layers and Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH='C:\\\\Users\\\\kevinmat\\\\Downloads\\\\kevin-ai-ml\\\\kevin-ai-ml\\\\Capstone\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "j6BBhYjPG58P",
    "outputId": "3a67d314-611e-4f76-cf87-f62952047b95"
   },
   "outputs": [],
   "source": [
    "# define iou or jaccard loss function\n",
    "def iou_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    score = (intersection + 1.) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection + 1.)\n",
    "    return 1 - score\n",
    "\n",
    "# combine bce loss and iou loss\n",
    "def iou_bce_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return 0.5 * keras.losses.binary_crossentropy(y_true, y_pred) + 0.5 * iou_loss(y_true, y_pred)\n",
    "\n",
    "# mean iou as a metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    intersect = tf.reduce_sum(y_true * y_pred, axis=[1, 2, 3])\n",
    "    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred, axis=[1, 2, 3])\n",
    "    smooth = tf.ones(tf.shape(intersect))\n",
    "    return tf.reduce_mean((intersect + smooth) / (union - intersect + smooth))\n",
    "\n",
    "\n",
    "# cosine learning rate annealing\n",
    "# changes learning rate based on the number of epocs passed\n",
    "def cosine_annealing(x):\n",
    "    lr = 0.001\n",
    "    epochs = 25\n",
    "    return lr* (np.cos(np.pi*x/epochs)+1.) /2\n",
    "learning_rate = tf.keras.callbacks.LearningRateScheduler(cosine_annealing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7SeZ99VedQ-"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'C:\\\\Users\\\\kevinmat\\\\Downloads\\\\kevin-ai-ml\\\\kevin-ai-ml\\\\Capstone\\stage_2_train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH=PROJECT_PATH + \"/stage_2_train_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_1 = generator(TRAIN_PATH, train_filenames, pneumonia_locations, batch_size=32, image_size=image_dimension, shuffle=False, augment=False, predict=False)\n",
    "valid_gen_1 = generator(TRAIN_PATH, valid_filenames, pneumonia_locations, batch_size=32, image_size=image_dimension, shuffle=False, predict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs1 = keras.Input(shape=(128, 128, 1))\n",
    "x1 = keras.layers.Conv2D(32, 3, padding='same', use_bias=False)(inputs1)\n",
    "    # residual blocks (4*4 downsample + 4*2*6 resblock = 64 layers)\n",
    "# output - 4 layers\n",
    "#x1 = keras.layers.BatchNormalization(momentum=0.9)(x1)\n",
    "x1 = keras.layers.LeakyReLU(0)(x1)\n",
    "#x1 = keras.layers.Flatten()(x1)\n",
    "outputs1 = keras.layers.Conv2D(1, 1, activation='sigmoid')(x1)\n",
    "#outputs1 = keras.layers.UpSampling2D(2**4)(x1)\n",
    "model12 = keras.Model(inputs=inputs1, outputs=outputs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_18 (InputLayer)       [(None, 128, 128, 1)]     0         \n",
      "                                                                 \n",
      " conv2d_99 (Conv2D)          (None, 128, 128, 32)      288       \n",
      "                                                                 \n",
      " leaky_re_lu_72 (LeakyReLU)  (None, 128, 128, 32)      0         \n",
      "                                                                 \n",
      " conv2d_100 (Conv2D)         (None, 128, 128, 1)       33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 321\n",
      "Trainable params: 321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model12.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "model12.compile(optimizer = 'adam', loss = iou_bce_loss, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/583 [===========>..................] - ETA: 32:52 - loss: 0.6027 - accuracy: 0.9731"
     ]
    }
   ],
   "source": [
    "history12 = model12.fit(train_gen_1, validation_data=valid_gen_1, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1olIXIX68zrD/5NnnQQUD",
   "include_colab_link": true,
   "name": "CapstoneGroup5-CNN_Based_Achitecture.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
